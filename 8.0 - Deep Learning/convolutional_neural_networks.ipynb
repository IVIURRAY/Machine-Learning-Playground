{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 1\n",
    "Convolution - Take a big matrix of pixels and simply it down to smaller matrix.\n",
    "    This is called a feature map. Big image and matrix of pixels, take a rolling\n",
    "    window of 3x3 and sum pixels and then it creates a feature map and in turn\n",
    "    a smaller matrix.\n",
    "    \n",
    "    Take many feature maps by trying to identify different features.\n",
    "    This makes lots of convolutional layers, then we can work out what features\n",
    "    make up this image.\n",
    "    \n",
    "     Convulations/ this process is also used for bluring and sharpening images.\n",
    "     As we apply different convulation matrixes to the original image and it\n",
    "     will change how the image looks.\n",
    "     \n",
    "     Google: Feature detectors.\n",
    "\n",
    "Step 2\n",
    "Max Pooling/Down smapling - Attempts to find the generic features in images to\n",
    "    identify features.\n",
    "    Otherwise, things like lighting, shaddow and angle of head would throw off \n",
    "    result that we put into the network.\n",
    "    Similar process to convluation but operates on feature maps. \n",
    "    Take the max value to avoid image distortion/lighting/shaddow (mroe generic).\n",
    "    \n",
    "    Google: scs.ryerson.ca/~aharley/vis/conv/flat.html\n",
    "    \n",
    "Step 3\n",
    "Flattening - Take the pooled feature map and turn into a 1xN array. Vector of inputs.\n",
    "\n",
    "Step 4 \n",
    "Full Connection - Use the flattening arrays as input to the network.\n",
    "\n",
    "\n",
    "Softmax & Cross-Entropy - Bring output values between 0-1 to get a probability.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librarys\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "IMG_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0802 16:57:50.967326 11152 deprecation.py:506] From h:\\code\\machine-learning\\machine-learning-playground\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Convolution\n",
    "# help(Convolution2D)\n",
    "model.add(\n",
    "    Convolution2D(\n",
    "        filters=32, # how many feature maps we want\n",
    "        kernel_size=3, # size of rolling window\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3), # tje shape and RGB of the images\n",
    "        activation='relu', # activation function\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Pooling\n",
    "# help(MaxPooling2D)\n",
    "model.add(\n",
    "    MaxPooling2D(\n",
    "        pool_size=(2, 2), # half on x & y the feature maps to pooling martix\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second convolution layer for increased accuracy\n",
    "model.add(\n",
    "    Convolution2D(\n",
    "        filters=32, # how many feature maps we want\n",
    "        kernel_size=3, # size of rolling window\n",
    "        activation='relu', # activation function\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    MaxPooling2D(\n",
    "        pool_size=(2, 2), # half on x & y the feature maps to pooling martix\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Flattening\n",
    "# help(Flatten\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Full Connection\n",
    "# help(Dense)\n",
    "model.add(\n",
    "    Dense(\n",
    "        128, # number of neurons (~average between no inputs and outcomes)\n",
    "        activation='relu', # activation function\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer\n",
    "model.add(\n",
    "    Dense(\n",
    "        1, # number of neurons (~average between no inputs and outcomes)\n",
    "        activation='sigmoid', # activation function - binary func\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 16:57:51.093325 11152 deprecation.py:323] From h:\\code\\machine-learning\\machine-learning-playground\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Compiling the CNN\n",
    "model.compile(\n",
    "    optimizer='adam', # how to optimse the netwrok\n",
    "    loss='binary_crossentropy', # loss function\n",
    "    metrics=['accuracy'] # what we're interested in\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 8000 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "8000/8000 [==============================] - 358s 45ms/step - loss: 0.3946 - acc: 0.8127 - val_loss: 0.1624 - val_acc: 0.9375\n",
      "Epoch 2/25\n",
      "8000/8000 [==============================] - 360s 45ms/step - loss: 0.1496 - acc: 0.9400 - val_loss: 0.0407 - val_acc: 0.9881\n",
      "Epoch 3/25\n",
      "8000/8000 [==============================] - 367s 46ms/step - loss: 0.0750 - acc: 0.9728 - val_loss: 0.0295 - val_acc: 0.9894\n",
      "Epoch 4/25\n",
      "8000/8000 [==============================] - 353s 44ms/step - loss: 0.0506 - acc: 0.9822 - val_loss: 0.0151 - val_acc: 0.9942\n",
      "Epoch 5/25\n",
      "8000/8000 [==============================] - 351s 44ms/step - loss: 0.0390 - acc: 0.9863 - val_loss: 0.0263 - val_acc: 0.9890\n",
      "Epoch 6/25\n",
      "8000/8000 [==============================] - 350s 44ms/step - loss: 0.0326 - acc: 0.9887 - val_loss: 0.0333 - val_acc: 0.9889\n",
      "Epoch 7/25\n",
      "8000/8000 [==============================] - 351s 44ms/step - loss: 0.0292 - acc: 0.9900 - val_loss: 0.0521 - val_acc: 0.9823\n",
      "Epoch 8/25\n",
      "8000/8000 [==============================] - 352s 44ms/step - loss: 0.0251 - acc: 0.9916 - val_loss: 0.0262 - val_acc: 0.9925\n",
      "Epoch 9/25\n",
      "8000/8000 [==============================] - 350s 44ms/step - loss: 0.0239 - acc: 0.9922 - val_loss: 0.0175 - val_acc: 0.9948\n",
      "Epoch 10/25\n",
      "8000/8000 [==============================] - 350s 44ms/step - loss: 0.0202 - acc: 0.9935 - val_loss: 0.0041 - val_acc: 0.9988\n",
      "Epoch 11/25\n",
      "8000/8000 [==============================] - 351s 44ms/step - loss: 0.0192 - acc: 0.9937 - val_loss: 0.0058 - val_acc: 0.9985\n",
      "Epoch 12/25\n",
      "8000/8000 [==============================] - 352s 44ms/step - loss: 0.0177 - acc: 0.9943 - val_loss: 0.0081 - val_acc: 0.9969\n",
      "Epoch 13/25\n",
      "8000/8000 [==============================] - 351s 44ms/step - loss: 0.0168 - acc: 0.9947 - val_loss: 0.0072 - val_acc: 0.9978\n",
      "Epoch 14/25\n",
      "8000/8000 [==============================] - 351s 44ms/step - loss: 0.0152 - acc: 0.9952 - val_loss: 0.0075 - val_acc: 0.9983\n",
      "Epoch 15/25\n",
      "8000/8000 [==============================] - 351s 44ms/step - loss: 0.0152 - acc: 0.9952 - val_loss: 0.0071 - val_acc: 0.9975\n",
      "Epoch 16/25\n",
      "8000/8000 [==============================] - 353s 44ms/step - loss: 0.0130 - acc: 0.9958 - val_loss: 0.0084 - val_acc: 0.9971\n",
      "Epoch 17/25\n",
      "8000/8000 [==============================] - 351s 44ms/step - loss: 0.0128 - acc: 0.9961 - val_loss: 0.0079 - val_acc: 0.9977\n",
      "Epoch 18/25\n",
      "8000/8000 [==============================] - 351s 44ms/step - loss: 0.0112 - acc: 0.9966 - val_loss: 0.0063 - val_acc: 0.9979\n",
      "Epoch 19/25\n",
      "8000/8000 [==============================] - 352s 44ms/step - loss: 0.0128 - acc: 0.9959 - val_loss: 0.0128 - val_acc: 0.9967\n",
      "Epoch 20/25\n",
      "8000/8000 [==============================] - 354s 44ms/step - loss: 0.0106 - acc: 0.9966 - val_loss: 0.0085 - val_acc: 0.9977\n",
      "Epoch 21/25\n",
      "8000/8000 [==============================] - 352s 44ms/step - loss: 0.0111 - acc: 0.9967 - val_loss: 0.0094 - val_acc: 0.9980\n",
      "Epoch 22/25\n",
      "8000/8000 [==============================] - 352s 44ms/step - loss: 0.0109 - acc: 0.9967 - val_loss: 0.0026 - val_acc: 0.9989\n",
      "Epoch 23/25\n",
      "8000/8000 [==============================] - 352s 44ms/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0039 - val_acc: 0.9990\n",
      "Epoch 24/25\n",
      "8000/8000 [==============================] - 354s 44ms/step - loss: 0.0102 - acc: 0.9969 - val_loss: 0.0265 - val_acc: 0.9932\n",
      "Epoch 25/25\n",
      "8000/8000 [==============================] - 351s 44ms/step - loss: 0.0100 - acc: 0.9971 - val_loss: 0.0105 - val_acc: 0.9971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x211b2d20cc0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the CNN to the images\n",
    "# changes the images to stop overfitting - like stretching, rotating and zooming images. \n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "root = 'H:/Code/machine-learning/Machine-Learning-Playground/1.0 - Example Data/Part 8 - Deep Learning/Convolutional_Neural_Networks/'\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        root + 'dataset/training_set',\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=25,\n",
    "        class_mode='binary'\n",
    ")\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        root + 'dataset/training_set',\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=25,\n",
    "        class_mode='binary'\n",
    ")\n",
    "\n",
    "# THIS TAKES LIKE >2hrs!!!\n",
    "model.fit_generator(\n",
    "        training_set,\n",
    "        steps_per_epoch=8000,\n",
    "        epochs=25,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_acc: 0.9971"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
